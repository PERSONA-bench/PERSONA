# Persona Benchmark â€“ Quickâ€‘Start Guide

A stepâ€‘byâ€‘step walkthrough for reproducing the **Personalizedâ€¯Conversationalâ€¯Benchmark** experiments contained in this repository.

---

## 1Â Â·Â Project layout

```
.
â”œâ”€â”€ demo/                      # toy sample used in the paper
â”‚Â Â  â”œâ”€â”€ demo.json
â”‚Â Â  â””â”€â”€ â€¦
â”œâ”€â”€ LLMEvaluators/             # common evaluation utilities (imported by GPT*.py)
â”œâ”€â”€ PromptMakers/              # reusable promptâ€‘building modules
â”œâ”€â”€ 3.1PromptMaker.py          # TaskÂ 3â€¯.1 â€“ sentiment classification prompt generator
â”œâ”€â”€ 3.2PromptMaker.py          # TaskÂ 3â€¯.2 â€“ exactâ€‘score regression prompt generator
â”œâ”€â”€ 3.3PromptMaker.py          # TaskÂ 3â€¯.3 â€“ nextâ€‘reply generation prompt generator
â”œâ”€â”€ GPT3.1.py                  # AzureÂ OpenAI evaluator for TaskÂ 3â€¯.1
â”œâ”€â”€ GPT3.2.py                  # AzureÂ OpenAI evaluator for TaskÂ 3â€¯.2
â”œâ”€â”€ GPT3.3.py                  # AzureÂ OpenAI evaluator for TaskÂ 3â€¯.3 /â€¯3â€¯.4
â”œâ”€â”€ requirements.txt           # all runtime dependencies
â””â”€â”€ README.md                  # â† **you are here**
```

---

## 2Â Â·Â Prerequisites

| Requirement       | Version tested | Notes                                                                    |
| ----------------- | -------------- | ------------------------------------------------------------------------ |
| **Python**        | 3.9Â â€“Â 3.11     | 3.10 recommended                                                         |
| **gitÂ +Â GitÂ LFS** | â‰¥â€¯2.39         | needed to pull the dataset                                               |
| **AzureÂ OpenAI**  | any paid tier  | for evaluation; you can swap in OpenAIÂ Cloud by adapting `MODEL_CONFIGS` |

> **GPUâ€¯support** is optional.Â `torch==2.6.0+cu118` (in *requirements.txt*) automatically installs CUDAâ€‘enabled wheels if a compatible NVIDIA driver is present.

---

## 3Â Â·Â Environment setup

<details>
<summary><strong>ğŸÂ Create & activate a virtual environment</strong></summary>

### Unix / macOS â€“ <em>venv</em>

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### WindowsÂ â€“Â <em>PowerShell</em>

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

</details>

### Install Python dependencies

```bash
python -m pip install --upgrade pip
pip install -r requirements.txt
```

---

## 4Â Â·Â Download the dataset

```bash
git lfs install
# Downloads into ./data/Personalized_Conversational_Benchmark
git clone https://huggingface.co/datasets/ShawnLi0415/Personalized_Conversational_Benchmark \
      data/Personalized_Conversational_Benchmark
```

If you prefer the **Hugging Faceâ€¯Hub CLI**:

```bash
pip install --upgrade huggingface_hub
huggingface-cli login          # optional for public datasets
huggingface-cli download \
       ShawnLi0415/Personalized_Conversational_Benchmark \
       --local-dir data/Personalized_Conversational_Benchmark
```

---

## 5Â Â·Â Configure LLM credentials

All GPTÂ scripts read their keys from the `MODEL_CONFIGS` dictionary.Â The cleanest approach is to expose the values as environment variables and patch the scripts to reference them:

```bash
# Bash / zsh
export AZURE_OPENAI_ENDPOINT="https://<yourâ€‘resource>.openai.azure.com/"
export AZURE_OPENAI_API_KEY="skâ€‘REPLACE_ME"
export AZURE_OPENAI_API_VERSION="2024-02-15-preview"
```

Then, inside *GPT*.py, replace the empty strings:

```python
MODEL_CONFIGS = {
    "gpt": {
        "api_type": "azure",
        "api_key":  os.getenv("AZURE_OPENAI_API_KEY"),
        "azure_endpoint": os.getenv("AZURE_OPENAI_ENDPOINT"),
        "api_version":    os.getenv("AZURE_OPENAI_API_VERSION"),
        "deployment_name": "gpt"
    }
}
```

---

## 6Â Â·Â Generate prompts

> **Tip:** Each *PromptMaker* script hardâ€‘codes its input JSON path.Â Either edit the constant or symlink the desired split to `demo/demo.json`.

```bash
## TaskÂ 3â€¯.1 â€“ sentiment classification
python 3.1PromptMaker.py

## TaskÂ 3â€¯.2 â€“ exact score prediction
python 3.2PromptMaker.py

## TaskÂ 3â€¯.3 â€“ nextâ€‘reply body generation
python 3.3PromptMaker.py
```

Outputs will appear in the project root:

```
WithConversationPrompts_ScorePrediction_Refactored.jsonl
WithoutConversationPrompts_ScorePrediction_Refactored.jsonl
WithConversationPrompts_ExactScorePrediction.jsonl # For Rand Experiments
â€¦ etc.
```

---

## 7Â Â·Â Run evaluation

After prompts are created and the API keys are in place:

```bash
## TaskÂ 3â€¯.1 (binary sentiment)
python GPT3.1.py --model gpt  # additional CLI flags are accepted

## TaskÂ 3â€¯.2 (regression)
python GPT3.2.py             # reads from With/Without *.jsonl automatically

## TaskÂ 3â€¯.3 /Â 3â€¯.4 (generation & multiâ€‘metric eval)
python GPT3.3.py             # long run; produces a detailed log & summary
```

All evaluators write timestamped logs plus metric summaries to the working directory.

---

## 8Â Â·Â Oneâ€‘command quickâ€‘demo

CreateÂ `run_demo.sh` in the repo root:

```bash
#!/usr/bin/env bash
set -euo pipefail

# 1Â â€“Â Generate all prompts
for t in 3.1 3.2 3.3; do
  python "${t}PromptMaker.py"
done

# 2Â â€“Â Evaluate with a single GPT deployment
for t in 3.1 3.2 3.3; do
  python "GPT${t}.py"
done
```

```bash
chmod +x run_demo.sh
./run_demo.sh
```

The script finishes with three CSV/JSONL metric reports in the current directory.

---

## 9Â Â·Â Troubleshooting

| Symptom                             | Checklist                                                                                            |
| ----------------------------------- | ---------------------------------------------------------------------------------------------------- |
| `ModuleNotFoundError`               | Did you **activate** the virtual environment and run `pip install -r requirements.txt`?              |
| `openai.RateLimitError`             | Verify your Azure quota.Â Use smaller batches or add `time.sleep()` in evaluator loops.               |
| `FileNotFoundError: demo/demo.json` | Point the *PromptMaker* `INPUT_JSON_FILE` constant to an existing split or symlink the desired JSON. |

---
... 